{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18239d9e-e562-47a0-9b99-d83d089db6f2",
   "metadata": {
    "collapsed": false,
    "name": "setup_md"
   },
   "source": [
    "# **SNOWFLAKE CORTEX COMPLETE FINANCIAL SERVICES DEMO**\n",
    "\n",
    "## Authors: John Heisler, Garrett Frere\n",
    "\n",
    "In this demo, using Snowflake Cortex (https://www.snowflake.com/en/data-cloud/cortex/), we will build an AI-infused Data Pipeline with Cortex Complete.\n",
    "\n",
    "### AI Pipeline Overview\n",
    "\n",
    "We'll learn how to extract raw text from a PDF, perform prompt engineering, and pass custom prompts and data to a large language model of our choosing all without leaving Snowflake.\n",
    "\n",
    "Specifically, we will be taking on the role of an AI Engineer who is working closely with a portfolio team at an asset manager. The portfolio team would like to speed up their ingestion and comprehension of statements by the Federal Open Market Committee (FOMC) who determines the direction of monetary policy by directing open market operations. Ultimately they would like to get a signal as to whether interest rates will increase, remain the same, or increase (hawkish, or, dovish respectively).\n",
    "\n",
    "I refer to this as an AI pipeline because we can imbue this type of signal generation with AI much further up the data delivery value chain. In this way, we will maximize the value of our work imbuing into a common dataset. End users will not need invoke any additional logic; good design is invisible!\n",
    "\n",
    "### Next Steps\n",
    "\n",
    " * To industrialize this demo with continuous ingestion and scoring, please check out the `FSI_Cortex_AI_Pipeline_Industrialization.ipynb` notebook in this repository\n",
    " * Check out the companion demo in this repository: `FSI_Cortex_Search.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b205a59",
   "metadata": {},
   "source": [
    "# üõë BEFORE YOU START üõë\n",
    "\n",
    "**Be sure to run the `1_SQL_SETUP_FOMC.sql` script to create dependent database objects for the following steps**\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5863ec-4a54-42b2-8dea-2c591475f30e",
   "metadata": {
    "collapsed": false,
    "name": "STEP_1_MD"
   },
   "source": [
    "### AI Pipeline: Step 1 - Create File Extraction Function\n",
    "\n",
    "We need to extract text from the PDFs. We will do that with a new python function.\n",
    "\n",
    "> Note that we're builidng this function directly in SQL.\n",
    "\n",
    "The steps below requires the `langchain`, `pypdf2` and `pandas` packages. To import packages from Anaconda, install them first using the package selector at the top of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b5cc86-6cb7-4978-85ea-5721582f6d60",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "step2_python"
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.snowpark.types import *\n",
    "from snowflake.snowpark.files import SnowflakeFile\n",
    "import PyPDF2, io, logging\n",
    "import pandas as pd\n",
    "\n",
    "# Get the active Snowflake session\n",
    "session = get_active_session()\n",
    "\n",
    "\n",
    "def PDF_TEXT_EXTRACTOR(file_url: str) -> str:\n",
    "    # Set up logging\n",
    "    logger = logging.getLogger(\"udf_logger\")\n",
    "    logger.info(f\"Opening file {file_url}\")\n",
    "\n",
    "    # Open the file from Snowflake stage\n",
    "    with SnowflakeFile.open(file_url, 'rb') as f:\n",
    "        buffer = io.BytesIO(f.readall())\n",
    "\n",
    "    # Initialize PDF reader\n",
    "    reader = PyPDF2.PdfReader(buffer)\n",
    "    text = \"\"\n",
    "\n",
    "    # Extract text from each page of the PDF\n",
    "    for page in reader.pages:\n",
    "        try:\n",
    "            # Extract text from the page, replace newlines and null characters with spaces\n",
    "            text += page.extract_text().replace('\\n', ' ').replace('\\0', ' ')\n",
    "        except:\n",
    "            # If extraction fails, log a warning and set text to \"Unable to Extract\"\n",
    "            text = \"Unable to Extract\"\n",
    "            logger.warning(f\"Unable to extract from file {file_url}, page {page}\")\n",
    "\n",
    "    return text\n",
    "\n",
    "# Add required packages to the session\n",
    "session.add_packages(\"snowflake-snowpark-python\", \"snowflake-ml-python\", \"snowflake\", \"PyPDF2\", \"pandas\")\n",
    "\n",
    "# Register the UDF (User Defined Function) in Snowflake\n",
    "session.udf.register(\n",
    "    func = PDF_TEXT_EXTRACTOR,\n",
    "    return_type = StringType(),\n",
    "    input_types = [ StringType()],\n",
    "    is_permanent = True,\n",
    "    name = 'PDF_TEXT_EXTRACTOR',\n",
    "    replace = True,\n",
    "    stage_location = '@gen_ai_fsi.fomc.fed_logic'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dc26a7-e011-4ad5-aad0-a963b146cb96",
   "metadata": {
    "collapsed": false,
    "name": "STEP_2_MD"
   },
   "source": [
    "### AI Pipeline: Step 2 - Create and Register `generate_prompt` Function\n",
    "\n",
    "As we load data into our system, we want to automatically generate a signal. To do so, we need to call an LLM and pass it our prompt. \n",
    "\n",
    "Below, we define our specialized prompt engineering as a python function and then we register the function for later reuse when loading data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32b66ad-ef51-495f-81ed-4de04af18fad",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "STEP_2_PYTHON"
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.snowpark.types import *\n",
    "\n",
    "session = get_active_session() \n",
    "\n",
    "def generate_prompt(document_text):\n",
    "    prompt = f\"\"\"\n",
    "        <Role> You are an experienced Senior Economist deeply knowledgeable on Federal Reserve guidance including FOMC or Federal Open Market Committee meeting minutes and communications.\n",
    "        You are an expert in interpreting Hawkish and Dovish signals from the Fed or Federal Reserve. Such signals are derived from guidance conveyed in FOMC meeting notes and communications.\n",
    "        \n",
    "        As an analyst, you excel at discerning macroeconomic trends for each FOMC meeting notes and communications published by the Federal Reserve.\n",
    "        The  signal or trends are either Hawkish or Dovish based on the growth outlook and inflation outlook of the Fed. The Federal Reserve has a long \n",
    "        term objective of keeping inflation around 2%, and low unemployment. Hawkish sentiment could imply \n",
    "        the Federal Reserve intends to raise interest rates to increase the cost of borrowing and slow economic activity. \n",
    "        The Fed typically increases interest rates when inflation is high or rising, or when the unemployment \n",
    "        rate is low or falling. Conversely, dovish sentiment could imply the Federal Reserve intends to lower interest \n",
    "        rates to allow easier access borrowing and lowering the cost of money to stimulate economic activity.  The Fed \n",
    "        typically decreases interest rates when inflation is low or falling, or when the unemployment rate is high or rising.\n",
    "        \n",
    "        Signal categories known as Economic Policy Stances:\n",
    "        Hawkish stance or attitude for economic policy\n",
    "        -characterized by a focus on combating inflation and often involves advocating for higher interest rates and tolerant to higher levels of unemployment.\n",
    "        -concerned about rising inflation. Hawkish stance believes higher interest rates can help keep inflation in check, even if it slows down economic growth or increases unemployment.\n",
    "        \n",
    "        Dovish stance or attitude for economic policy\n",
    "        -characterized by a focus on prioritizing stimulating economic growth, reducing unemployment, and tolerant to higher levels of inflation.\n",
    "        -concerned with boosting economic activity, reducing unemployment and, for this reason, lower interest rates are preferred to create economic growth and employment.\n",
    "        \n",
    "        Neutral stance or attitude for economic policy\n",
    "        -characterized by a focus on balance between combating inflation and supporting economic growth, with no strong inclination toward either side.\n",
    "        -concerned with maintaining a steady economic environment without significant deviations. They seek to neither overly stimulate the economy nor excessively tighten it.\n",
    "        </Role>\n",
    "        \n",
    "        <Data> \n",
    "        You are provided the text of a Federal Reserve Guidance or FOMC meeting notes as context. These generally are released before the Federal Reserve takes action on economic policy. \n",
    "        </Data>\n",
    "\n",
    "        <FOMC_meeting_notes>\n",
    "        {document_text}\n",
    "        </FOMC_meeting_notes>\n",
    "        \n",
    "        <Task>: Follow these instructions,\n",
    "        1) Review the provided FOMC communication or meeting notes text. Then,\n",
    "        2) Consider the FOMC members or Committee Members tone and sentiment around economic conditions. Then,\n",
    "        3) Consider specific guidance and stated conditions that validate the tone and signal FOMC members make concerning current macro economic conditions. Then,\n",
    "        4) Based on this sentiment classify if the FOMC communication text indicates Hawkish, Dovish, or Neutral outlook for the economy. Be critical and do not categorize sentiment as \"Neutral\" unless necessary. This will be output as [Signal].\n",
    "        5) Summarize a concise and accurate rationale for classifying the sentiment Hawkish, Neutral, or Dovish sentiment. This will be output as [Signal_Summary].\n",
    "        </Task>\n",
    "        \n",
    "        <Output> \n",
    "        produce valid JSON. Absolutely do not include any additional text before or following the JSON. Output should use following JSON_format\n",
    "        </Output>\n",
    "        \n",
    "        <JSON_format>\n",
    "        {{\n",
    "            \"Signal\": (A trend sentiment classification of Hawkish, Neutral or Dovish),\n",
    "            \"Signal_Summary\": (A concise summary of sentiment trend),\n",
    "        }}\n",
    "        </JSON_format>\"\"\"\n",
    "    return prompt\n",
    "\n",
    "session.add_packages(\"snowflake-snowpark-python\", \"snowflake-ml-python\", \"snowflake\")\n",
    "\n",
    "session.udf.register(\n",
    "  func = generate_prompt,\n",
    "  return_type = StringType(),\n",
    "  input_types = [ StringType()],\n",
    "  is_permanent = True,\n",
    "  name = 'generate_prompt',\n",
    "  replace = True,\n",
    "  stage_location = '@gen_ai_fsi.fomc.fed_logic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba82eaad-873c-435a-9026-551ec0fc61ac",
   "metadata": {
    "collapsed": false,
    "name": "STEP_3_MD"
   },
   "source": [
    "### AI Pipeline: Step 3 - Ingest Text and Determine Signal\n",
    "\n",
    "Now we're using the functions that we've just created in a simple insert statement. This approach of encapsulating complexity for later reuse in SQL pipelines greatly increases the value of our work in a one-to-many relationship.\n",
    "\n",
    "### ü§Ø üß† CHECK IT OUT! üß† ü§Ø \n",
    "* We're calling our pdf text extractor function! (line 11)\n",
    "* We're calling our promp function! (line 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a179c8-5a2b-4377-a179-6dd613ad95f6",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "STEP_3_SQL"
   },
   "outputs": [],
   "source": [
    "INSERT INTO gen_ai_fsi.fomc.pdf_full_text (id, relative_path, size, last_modified, md5, etag, file_url, file_text, file_date, signal_mis)\n",
    "WITH cte AS (SELECT gen_ai_fsi.fomc.fed_pdf_full_text_sequence.nextval AS id,\n",
    "                    relative_path                                      AS relative_path,\n",
    "                    size                                               AS size,\n",
    "                    last_modified                                      AS last_modified,\n",
    "                    md5                                                AS md5,\n",
    "                    etag                                               AS etag,\n",
    "                    file_url                                           AS file_url,\n",
    "                    REPLACE(gen_ai_fsi.fomc.pdf_text_extractor(build_scoped_file_url('@gen_ai_fsi.fomc.fed_pdf', relative_path)), '''', '')  AS file_text,\n",
    "                    TRY_TO_DATE(REGEXP_SUBSTR(relative_path, '\\\\d{8}'), 'YYYYMMDD') AS file_date\n",
    "             FROM fomc_stream\n",
    "             WHERE metadata$action = 'INSERT')\n",
    "\n",
    "SELECT id,\n",
    "       relative_path,\n",
    "       size,\n",
    "       last_modified,\n",
    "       md5,\n",
    "       etag,\n",
    "       file_url,\n",
    "       file_text,\n",
    "       file_date,\n",
    "       snowflake.cortex.try_complete('mistral-large2', gen_ai_fsi.fomc.generate_prompt(file_text)) AS signal_mis\n",
    "FROM cte;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62fc411-3a75-4fc5-8754-e138c8315b2a",
   "metadata": {
    "collapsed": false,
    "name": "STEP_3SQL"
   },
   "source": [
    "### AI Pipeline: Step 3.1 - Check out the result\n",
    "\n",
    "select from our PDF table to view our signal and a summary or reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adcbc96-fdce-48d8-ada8-ab8c8ec1d9d2",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "cell2"
   },
   "outputs": [],
   "source": [
    "select * from GEN_AI_FSI.FOMC.PDF_FULL_TEXT;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9378a99-170b-4bbb-96cb-7c516e13f105",
   "metadata": {
    "collapsed": false,
    "name": "md_RAG"
   },
   "source": [
    "-------\n",
    "\n",
    "## Build a RAG Interface on FOMC Documents\n",
    "\n",
    "Awesome, we have created the pipeline to ingest and generate a signal when new data is avaiable -- this is our **AI pipeline**. \n",
    "\n",
    "Next, let's also give our users a means to ask more detailed questions about the content in the documents with a RAG interface. We'll use Cortex Search and the data we already have in the Stage as a foundation. \n",
    "\n",
    "In this section we'll: \n",
    "1. Create a new table function to chunk the pdfs.\n",
    "2. Use the chunking function to break the text into chunks and load it into our table.\n",
    "3. Create a Cortex Search Service to handle the vectorization and search functionality.\n",
    "4. Create a Chat interface with Streamlit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b3df6a-2005-444f-9f36-edf5cc0d808a",
   "metadata": {
    "collapsed": false,
    "name": "md_chunking_function"
   },
   "source": [
    "## Chunking Function\n",
    "Earlier, we created a function that pulled all of the text out of a PDF. Now, we'll do somethign very similar but we're going to break the text up into chunks for fuel our RAG interface. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b10280d-bdaf-4b11-b194-932f0fc0015f",
   "metadata": {
    "language": "sql",
    "name": "sql_chunking_function"
   },
   "outputs": [],
   "source": [
    "--create a function to chunk the pdfs\n",
    "CREATE OR REPLACE FUNCTION gen_ai_fsi.fomc.pdf_text_chunker(file_url string)\n",
    "RETURNS TABLE (chunk varchar)\n",
    "LANGUAGE PYTHON\n",
    "RUNTIME_VERSION = '3.9'\n",
    "HANDLER = 'pdf_text_chunker'\n",
    "PACKAGES = ('snowflake-snowpark-python','PyPDF2', 'langchain')\n",
    "AS\n",
    "$$\n",
    "from snowflake.snowpark.types import StringType, StructField, StructType\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from snowflake.snowpark.files import SnowflakeFile\n",
    "import PyPDF2, io\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "class pdf_text_chunker:\n",
    "\n",
    "    def read_pdf(self, file_url: str) -> str:\n",
    "    \n",
    "        logger = logging.getLogger(\"udf_logger\")\n",
    "        logger.info(f\"Opening file {file_url}\")\n",
    "    \n",
    "        with SnowflakeFile.open(file_url, 'rb') as f:\n",
    "            buffer = io.BytesIO(f.readall())\n",
    "            \n",
    "        reader = PyPDF2.PdfReader(buffer)   \n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            try:\n",
    "                text += page.extract_text().replace('\\n', ' ').replace('\\0', ' ')\n",
    "            except:\n",
    "                text = \"Unable to Extract\"\n",
    "                logger.warn(f\"Unable to extract from file {file_url}, page {page}\")\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def process(self,file_url: str):\n",
    "\n",
    "        text = self.read_pdf(file_url)\n",
    "        \n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size = 500, #Adjust this as you see fit\n",
    "            chunk_overlap  = 50, #This let's text have some form of overlap. Useful for keeping chunks contextual\n",
    "            length_function = len\n",
    "        )\n",
    "    \n",
    "        chunks = text_splitter.split_text(text)\n",
    "        df = pd.DataFrame(chunks, columns=['chunks'])\n",
    "        \n",
    "        yield from df.itertuples(index=False, name=None)\n",
    "$$;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3702f0db-5abc-491a-aa57-891896cfa10b",
   "metadata": {
    "collapsed": false,
    "name": "md_chunking"
   },
   "source": [
    "## Build the Chunk Table\n",
    "\n",
    "Using our newly create chunking table function, bring in and chunk all of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026b24dc-1849-4c64-b0b3-337bc85781be",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "sql_chunk_pdfs"
   },
   "outputs": [],
   "source": [
    "TRUNCATE TABLE gen_ai_fsi.fomc.pdf_chunks;\n",
    "\n",
    "INSERT INTO gen_ai_fsi.fomc.pdf_chunks (id, full_text_fk, relative_path, file_date, chunk)\n",
    "WITH chunk_cte AS (SELECT gen_ai_fsi.fomc.fed_pdf_chunk_sequence.nextval AS id,\n",
    "                          relative_path,\n",
    "                          REPLACE(func.chunk, '''', '')  AS chunk\n",
    "                   FROM directory(@gen_ai_fsi.fomc.fed_pdf),\n",
    "                        TABLE(gen_ai_fsi.fomc.pdf_text_chunker(build_scoped_file_url(@gen_ai_fsi.fomc.fed_pdf, relative_path))) AS func)\n",
    "\n",
    "SELECT cte.id,\n",
    "       pft.id,\n",
    "       cte.relative_path,\n",
    "       pft.file_date,\n",
    "       cte.chunk\n",
    "FROM chunk_cte cte\n",
    "         LEFT JOIN gen_ai_fsi.fomc.pdf_full_text pft\n",
    "            ON cte.relative_path = pft.relative_path;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95175d3-e4c5-47aa-b21a-e504b6ac6e7f",
   "metadata": {
    "collapsed": false,
    "name": "md_search_service"
   },
   "source": [
    "## Create a Cortex Search Service\n",
    "\n",
    "Cortex Search enables low-latency, high-quality ‚Äúfuzzy‚Äù search over your Snowflake data. Cortex Search powers a broad array of search experiences for Snowflake users including Retrieval Augmented Generation (RAG) applications leveraging Large Language Models (LLMs).\n",
    "\n",
    "We'll use this service later to power our RAG application. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0576cec9-f819-4598-951a-b660a2a977e9",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "sql_search_service"
   },
   "outputs": [],
   "source": [
    "--create a cortex Search Service \n",
    "CREATE OR REPLACE CORTEX SEARCH SERVICE SRCH_FED\n",
    "ON CHUNK\n",
    "ATTRIBUTES ID, FILE_DATE\n",
    "WAREHOUSE = GEN_AI_FSI_WH\n",
    "TARGET_LAG = '1 day'\n",
    "AS (\n",
    "    SELECT \n",
    "        ID,\n",
    "        FILE_DATE::string as FILE_DATE,\n",
    "        CHUNK AS CHUNK  \n",
    "FROM PDF_CHUNKS);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b0e39c-62d3-494f-95ba-8279a88eedae",
   "metadata": {
    "collapsed": false,
    "name": "md_search"
   },
   "source": [
    "# FOMC Chat Interface\n",
    "* We're leveraging our Cortex Search service enabling users to ask targeted questions of the documents in their stage.\n",
    "* a robust chat interface could be built to handle this, for the demo, we have a bare bones interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3395e7b9-fb06-4cb0-ac10-ff007345e0fa",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cs_py_service"
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.core import Root\n",
    "import streamlit as st\n",
    "import json5 as json\n",
    "import pandas as pd\n",
    "#import snowflake.snowpark.modin.plugin\n",
    "\n",
    "#get our session\n",
    "session = get_active_session()\n",
    "\n",
    "# access search service through Python API\n",
    "root = Root(session)\n",
    "search_service = (root\n",
    "                  .databases[\"GEN_AI_FSI\"]\n",
    "                  .schemas[\"FOMC\"]\n",
    "                  .cortex_search_services[\"SRCH_FED\"]    \n",
    ")\n",
    "\n",
    "#create a function to generate response\n",
    "def complete_cs(model_name, prompt):\n",
    "    cmd = f\"\"\"SELECT SNOWFLAKE.CORTEX.TRY_COMPLETE('{model_name}','{prompt}') as response\"\"\"\n",
    "    df_response = session.sql(cmd).collect()\n",
    "    response = df_response[0].RESPONSE\n",
    "    return response\n",
    "\n",
    "\n",
    "#get FOMC files\n",
    "database = 'GEN_AI_FSI'\n",
    "schema = 'FOMC'\n",
    "\n",
    "#USER INPUT: select time frame\n",
    "query_document_dates = f\"\"\"SELECT DISTINCT FILE_DATE FROM {database}.{schema}.PDF_CHUNKS order by file_date desc;\"\"\"\n",
    "df_document_dates = session.sql(query_document_dates).to_pandas()\n",
    "\n",
    "#USER INPUT: select model\n",
    "query_models = f\"\"\"SELECT MODEL FROM {database}.{schema}.MODELS\"\"\"\n",
    "df_models = session.sql(query_models).to_pandas()\n",
    "\n",
    "#USER INPUT: display\n",
    "col1, col2 = st.columns(2)\n",
    "with col1:\n",
    "    user_input_date = st.selectbox(\"Select Document Date\", df_document_dates, key=\"CS_date_select_box\")\n",
    "with col2: \n",
    "    user_input_model = st.selectbox(\"Select Model\", df_models, key=\"CS_model_select_box\")\n",
    "\n",
    "#Generate a response\n",
    "user_input_question = st.text_input(\"Ask me a question\")\n",
    "\n",
    "ask= st.button(\"Ask\", key = \"button_ask\")\n",
    "if ask: \n",
    "    #get the cunks that are relevant to the question\n",
    "    response = search_service.search(\n",
    "        user_input_question,\n",
    "        columns=[\"ID\", \"FILE_DATE\", \"CHUNK\"],\n",
    "        filter={\"@eq\": {\"FILE_DATE\": f\"\"\"{user_input_date}\"\"\"} },\n",
    "        limit=5,\n",
    "    ).to_json()\n",
    "\n",
    "    #st.json(response)\n",
    "    # Parse the JSON5 string\n",
    "    context_chunks = json.loads(response)\n",
    "    \n",
    "    #transform the data into a single string\n",
    "    context_full = \"\"\n",
    "    for chunk in context_chunks['results']:\n",
    "        context_full += chunk['CHUNK'] + \" \"\n",
    "\n",
    "    #build our prompt\n",
    "    cs_prompt = f'''\n",
    "            Role: You are an expert Senior Economist deeply knowledgeable on Federal Reserve documents and guidance including FOMC or Federal Open Market Committee \n",
    "            meeting minutes and communications. You are an expert in interpreting and answering investment-related questions based on meeting minutes and communications \n",
    "            which you are provided as context with each question.\n",
    "            \n",
    "            Data: You are provided with relevant text of a Federal Reserve Guidance or FOMC meeting notes relenavt to the question asked. \n",
    "            These meeting notes are generally released before the Federal Reserve takes action on economic policy.\n",
    "            \n",
    "            Task: Follow these instructions,\n",
    "            1) Answer the question based on the context. \n",
    "            2) Be terse and do not consider information outside what you have been provided in the question and context.\n",
    "            \n",
    "            Output: produce thourough, valid, grammatically correct, and concise response in a professional tone. Please do not preface your response. also provide the document and location you used to answer the question.\n",
    "            Context: {context_full}\n",
    "            Question: Based on documents released on this date {user_input_date}, {user_input_question} \n",
    "            '''\n",
    "\n",
    "    data = complete_cs(user_input_model, cs_prompt)\n",
    "    \n",
    "    with st.chat_message(\"model\", avatar =\"assistant\"):\n",
    "        st.write(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18239d9e-e562-47a0-9b99-d83d089db6f2",
   "metadata": {
    "name": "md_steup",
    "collapsed": false
   },
   "source": "# **CORTEX AI FINANCIAL SERVICES HANDS-ON LAB**\n## Authors: John Heisler, Garrett Frere\n\n### Intention of This Lab\nThis notebook is meant to be a deep-dive into industrialzing functionlity in the companion AI Pipeline Demo in this repository, FSI_Cortex_AI_Pipeline.ipynb. Please run through the AI Pipeline notebook first to understand the functional elements of this lab. \n\n### AI Pipeline\nWe will build some python functions to automatically download the latest FOMC documents and load them directly into our stage (no need to land locally then PUT). Then we will use a combination of stored procedures, streams, and tasks to get that data into snowflake tables and ready to Query. In that pipeline, we will automatically generate a hawkish, dovish, or neutral sentiment. In this way, we will maximize the value of our work imbuing into a common dataset. End users will not need invoke any addition logic in plain SQL. ***Good design is invisible!***"
  },
  {
   "cell_type": "markdown",
   "id": "ed6def1c-7dbb-4b60-baf9-a2562eab5496",
   "metadata": {
    "collapsed": false,
    "name": "md_manual_data_loads"
   },
   "source": "# ðŸ›‘ --> BEFORE YOU START <-- ðŸ›‘\n\n1. **Run the 1_SQL_SETUP_FOMC.sql script.**\n2. **Enable our External Access**\n    1. In the top right corner of Snowsight click the three vertically aligned dots.\n    2. In the context menu, select Notebook Settings.\n    3. Select the External Access tab.\n    4. Select the toggle switch to the right of FED_RESERVE_ACCESS_INTEGRATION.\n3. **Install the appropriate packages in the notebook**\n    1. Click on the packages drop down in the top right of Snowsight. \n    2. select the folloiwng packages\n        a. bs4\n        b. joblib\n        c. json5\n        d. pandas\n        e. python-dotenv\n        f. requests\n        g. snowflake\n        h. snowflake-ml-python\n        i. snowflake-snowpark-python\n"
  },
  {
   "cell_type": "markdown",
   "id": "d549e7f4-5b03-478b-b079-f7e6273b2520",
   "metadata": {
    "collapsed": false,
    "name": "md_pdf_scrape"
   },
   "source": "## Download PDF Stored Procedure\n\nLet's build the logic to download the PDFs from the federal reserve website to our stage. We'll put that logic in a Python Stored Procedure which we will automate with a task."
  },
  {
   "cell_type": "code",
   "id": "68a53346-8f81-4882-8cc1-3c675cc10300",
   "metadata": {
    "language": "sql",
    "name": "sql_get_pdfs_sproc",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "CREATE OR REPLACE PROCEDURE get_fed_pdfs()\nRETURNS VARCHAR\nLANGUAGE PYTHON\nRUNTIME_VERSION = 3.9\nPACKAGES = ('snowflake-snowpark-python', 'requests', 'bs4', 'snowflake')\nEXTERNAL_ACCESS_INTEGRATIONS = (\"FED_RESERVE_ACCESS_INTEGRATION\")\nHANDLER='main'\nEXECUTE AS CALLER\nAS $$\n\nimport snowflake.snowpark as snowpark\nimport requests \nfrom bs4 import BeautifulSoup\nfrom io import BytesIO\n\ndef get_all_fomc_pdfs():\n    try:\n        response = requests.get('https://www.federalreserve.gov/monetarypolicy/fomccalendars.htm')\n        response.raise_for_status()\n        soup = BeautifulSoup(response.text, 'html.parser')\n        pdf_links = []\n        for link in soup.find_all('a', href=True):\n            href = link['href']\n            if href.endswith('.pdf'):\n                if not href.startswith('http'):\n                    href = 'https://www.federalreserve.gov' + href\n                pdf_links.append(href)\n        return pdf_links\n    except requests.RequestException as e:\n        print(f\"Error fetching the FOMC statement page: {e}\")\n        return ['no new links']\n\ndef main(session):\n    database = 'GEN_AI_FSI'\n    schema = 'FOMC'\n    stage = 'FED_PDF'\n\n    #return all the files on the website\n    pdfs = get_all_fomc_pdfs()\n    #return all the files in your stage\n    query = f'LIST @{database}.{schema}.{stage}'\n    stage_files = session.sql(query).collect()\n    stage_files = [row['name'] for row in stage_files]\n    #set the stage\n    stage = f'@{database}.{schema}.{stage}/'\n    \n    #Download the new files\n    for pdf in pdfs:\n        filename = pdf.split('/')[-1]\n        if 'fed_pdf/' + filename not in stage_files:\n            response = requests.get(pdf)\n            full_file_name = stage+filename\n            file_data = response.content\n            buffer = BytesIO(file_data)\n            session.file.put_stream(buffer, full_file_name, auto_compress=False, overwrite=False)\n            print(filename,':\\t downloading')\n        else:\n            print(filename,':\\t already downloaded')\n    return str('all data has been downloaded')\n$$;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "508eff2a-58e3-42e7-a800-e71ba1c17059",
   "metadata": {
    "collapsed": false,
    "name": "md_file_extraction"
   },
   "source": "# Create File Extraction Function\n\nWe need to extract the text from the PDFs. We will do that with a new python function"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb32c18c-b4ba-409e-a3ea-24222d9068c1",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "sql_file_extraction"
   },
   "outputs": [],
   "source": "--create a fucntion to simply extract the text as a single line\ncreate or replace function PDF_TEXT_EXTRACTOR(file_url string)\nreturns varchar\nlanguage python\nruntime_version = '3.9'\nhandler = 'read_pdf'\npackages = ('snowflake-snowpark-python','PyPDF2', 'langchain')\nas\n$$\nfrom snowflake.snowpark.types import StringType, StructField, StructType\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom snowflake.snowpark.files import SnowflakeFile\nimport PyPDF2, io\nimport logging\nimport pandas as pd\n\n\ndef read_pdf(file_url: str) -> str:\n\n    logger = logging.getLogger(\"udf_logger\")\n    logger.info(f\"Opening file {file_url}\")\n\n    with SnowflakeFile.open(file_url, 'rb') as f:\n        buffer = io.BytesIO(f.readall())\n        \n    reader = PyPDF2.PdfReader(buffer)   \n    text = \"\"\n    for page in reader.pages:\n        try:\n            text += page.extract_text().replace('\\n', ' ').replace('\\0', ' ')\n        except:\n            text = \"Unable to Extract\"\n            logger.warn(f\"Unable to extract from file {file_url}, page {page}\")\n    \n    return text\n$$;"
  },
  {
   "cell_type": "markdown",
   "id": "97dc26a7-e011-4ad5-aad0-a963b146cb96",
   "metadata": {
    "name": "md_generate_signal",
    "collapsed": false
   },
   "source": "# Create and Register generate_prompt Function\n\nAs we load data into our system, we want to automatically generate a signal. To do so, we need to call an LLM and pass it our prmpt. \n\nBelow, we build our specialized prompt engineering as a function and then we register the function for later reuse when loading data."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32b66ad-ef51-495f-81ed-4de04af18fad",
   "metadata": {
    "name": "py_generate_signal",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark.types import *\n\nsession = get_active_session() \n\ndef generate_prompt(document_text):\n    prompt = f\"\"\"\n        <Role> You are an experienced Senior Economist deeply knowledgeable on Federal Reserve guidance including FOMC or Federal Open Market Committee meeting minutes and communications.\n        You are an expert in interpreting Hawkish and Dovish signals from the Fed or Federal Reserve. Such signals are derived from guidance conveyed in FOMC meeting notes and communications.\n        \n        As an analyst, you excel at discerning macroeconomic trends for each FOMC meeting notes and communications published by the Federal Reserve.\n        The  signal or trends are either Hawkish or Dovish based on the growth outlook and inflation outlook of the Fed. The Federal Reserve has a long \n        term objective of keeping inflation around 2%, and low unemployment. Hawkish sentiment could imply \n        the Federal Reserve intends to raise interest rates to increase the cost of borrowing and slow economic activity. \n        The Fed typically increases interest rates when inflation is high or rising, or when the unemployment \n        rate is low or falling. Conversely, dovish sentiment could imply the Federal Reserve intends to lower interest \n        rates to allow easier access borrowing and lowering the cost of money to stimulate economic activity.  The Fed \n        typically decreases interest rates when inflation is low or falling, or when the unemployment rate is high or rising.\n        \n        Signal categories known as Economic Policy Stances:\n        Hawkish stance or attitude for economic policy\n        -characterized by a focus on combating inflation and often involves advocating for higher interest rates and tolerant to higher levels of unemployment.\n        -concerned about rising inflation. Hawkish stance believes higher interest rates can help keep inflation in check, even if it slows down economic growth or increases unemployment.\n        \n        Dovish stance or attitude for economic policy\n        -characterized by a focus on prioritizing stimulating economic growth, reducing unemployment, and tolerant to higher levels of inflation.\n        -concerned with boosting economic activity, reducing unemployment and, for this reason, lower interest rates are preferred to create economic growth and employment.\n        \n        Neutral stance or attitude for economic policy\n        -characterized by a focus on balance between combating inflation and supporting economic growth, with no strong inclination toward either side.\n        -concerned with maintaining a steady economic environment without significant deviations. They seek to neither overly stimulate the economy nor excessively tighten it.\n        </Role>\n        \n        <Data> \n        You are provided the text of a Federal Reserve Guidance or FOMC meeting notes as context. These generally are released before the Federal Reserve takes action on economic policy. \n        </Data>\n\n        <FOMC_meeting_notes>\n        {document_text}\n        </FOMC_meeting_notes>\n        \n        <Task>: Follow these instructions,\n        1) Review the provided FOMC communication or meeting notes text. Then,\n        2) Consider the FOMC members or Committee Members tone and sentiment around economic conditions. Then,\n        3) Consider specific guidance and stated conditions that validate the tone and signal FOMC members make concerning current macro economic conditions. Then,\n        4) Based on this sentiment classify if the FOMC communication text indicates Hawkish, Dovish, or Neutral outlook for the economy. Be critical and do not categorize sentiment as \"Neutral\" unless necessary. This will be output as [Signal].\n        5) Summarize a concise and accurate rationale for classifying the sentiment Hawkish, Neutral, or Dovish sentiment. This will be output as [Signal_Summary].\n        </Task>\n        \n        <Output> \n        produce valid JSON. Absolutely do not include any additional text before or following the JSON. Output should use following JSON_format\n        </Output>\n        \n        <JSON_format>\n        {{\n            \"Signal\": (A trend sentiment classification of Hawkish, Neutral or Dovish),\n            \"Signal_Summary\": (A concise summary of sentiment trend),\n        }}\n        </JSON_format>\"\"\"\n    return prompt\n\nsession.add_packages(\"snowflake-snowpark-python\", \"snowflake-ml-python\", \"snowflake\")\n\nsession.udf.register(\n  func = generate_prompt\n, return_type = StringType()\n, input_types = [ StringType()]\n, is_permanent = True\n, name = 'generate_prompt'\n, replace = True\n, stage_location = '@fed_logic')"
  },
  {
   "cell_type": "markdown",
   "id": "a68d4acc-e3ec-4d13-bdf2-6200bf7c88c3",
   "metadata": {
    "collapsed": false,
    "name": "md_pipeline"
   },
   "source": "# Build Data Pipeline\nWe have created our data structure and all of the logic we need to download and parse our unstrucuted data and generate a custom prompt. Next, let's automate the pipeline with Streams and Tasks.\n\nWe will follow most of the steps outlined in our documentation examples: https://docs.snowflake.com/en/user-guide/data-load-dirtables-pipeline.\n\n![image info](https://docs.snowflake.com/en/_images/data-lake-dirtable-pipeline.png)"
  },
  {
   "cell_type": "markdown",
   "id": "6b87243b-d241-47b6-a40b-85e5945ca5bf",
   "metadata": {
    "name": "md_pdf_download",
    "collapsed": false
   },
   "source": "## Automate Ingestion of PDFs\n1. Create a task to run every hour that will execute the stored procedure we just created.\n2. Create a stream to check for new PDFs in our stage.\n3. Create a task to ingest text by calling our PDF_TEXT_EXTRACTOR function directly in SQL."
  },
  {
   "cell_type": "markdown",
   "id": "bdece727-e7d0-4d51-b67b-62015441a672",
   "metadata": {
    "name": "md_download_fed_pds_to_stage",
    "collapsed": false
   },
   "source": "## 1. Create a task to execute the stored procedure that downloads the PDFs\nThis will execute the get_fed_pdfs stored procedure we just created-- remember, that just downloads the pdfs to our stage. "
  },
  {
   "cell_type": "code",
   "id": "6c86025a-db6d-4f3e-a9a7-f1b4780d93a5",
   "metadata": {
    "language": "sql",
    "name": "sql_download_fed_pds_to_stage",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TASK download_fed_pdf_to_stage\n\tWAREHOUSE=GEN_AI_FSI_WH\n\tSCHEDULE='60 minute' --every hour\n\tCOMMENT='Download new FOMC files to our stage from the fed website.'\n\tAS \n        CALL get_fed_pdfs();",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "21959204-3952-4f26-8eed-8a951f0b6f93",
   "metadata": {
    "name": "md_pipeline_create_stream",
    "collapsed": false
   },
   "source": "## 2. Create a stream to check for new PDFs in our stage\n* Streams record change data capture (CDC) and we'll use this stream to cdc our stage directory table.\n    * https://docs.snowflake.com/en/user-guide/streams-intro"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7349c096-b05b-4527-a299-b1e75eb0d0ae",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "sql_pipeline_create_stream",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE STREAM GEN_AI_FSI.FOMC.FOMC_STREAM on DIRECTORY(@FED_PDF);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba82eaad-873c-435a-9026-551ec0fc61ac",
   "metadata": {
    "name": "md_stage_to_snow",
    "collapsed": false
   },
   "source": "## 3. Create a task to ingest text \nTasks allow us to schedule particular actions in Snowflake. In this case, we're scheduling the ingestion of the FOMC PDF text and enriching it with AI-enabled signal. Remember the data is moving from our stage into our Snowflake tables.\n    \nLearn more about tasks: https://docs.snowflake.com/en/user-guide/tasks-intro\n\n\n### ðŸ¤¯ ðŸ§  CHECK IT OUT! ðŸ§  ðŸ¤¯ \n* We're calling our pdf text extractor python logic in SQL! (line 16)\n* We're calling our signal logic upon ingestion! (line 33)\n* We'll only load data from our stage to our table when the stream has a new record (remember the stream fills up with records of new data in the stage)\n\nNOTE: Weâ€™re only bringing in the top 1 result (line 9). This is to speed up the initial load by restricting load to a single file. If you would like to load all of the PDF data into a table, simply delete or comment that clasue out."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a179c8-5a2b-4377-a179-6dd613ad95f6",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "sql_pipeline_create_task",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TASK LOAD_FED_PDFS_STAGE_TO_TABLE\n\tWAREHOUSE=GEN_AI_FSI_WH\n\tSCHEDULE='720 minute' --check twice a day, will only execute if the stream has data\n\tCOMMENT='Process new FOMC files on the stage and insert their data into the PDF_DETAIL table.'\n\tWHEN SYSTEM$STREAM_HAS_DATA('FOMC_STREAM')\n\tAS INSERT INTO PDF_FULL_TEXT (\n    WITH CTE AS ( \n        SELECT TOP 1\n                FED_PDF_FULL_TEXT_SEQUENCE.NEXTVAL as ID,\n                RELATIVE_PATH,\n                SIZE,\n                LAST_MODIFIED,\n                MD5, \n                ETAG,\n                FILE_URL,\n                REPLACE(pdf_text_extractor(build_scoped_file_url('@FED_PDF', relative_path)),'''' ,'') as FILE_TEXT, \n                TRY_TO_DATE(REGEXP_SUBSTR (relative_path, '\\\\d{8}'),'YYYYMMDD') as FILE_DATE,\n            FROM \n                FOMC_STREAM\n            WHERE \n                METADATA$ACTION='INSERT'\n        )\n        SELECT\n            ID, \n            RELATIVE_PATH,\n            SIZE,\n            LAST_MODIFIED,\n            MD5, \n            ETAG,\n            FILE_URL,\n            FILE_TEXT, \n            FILE_DATE,\n            SNOWFLAKE.CORTEX.TRY_COMPLETE('mistral-large2', generate_prompt(FILE_TEXT)) AS SIGNAL_mis\n        FROM\n            CTE\n    );"
  },
  {
   "cell_type": "markdown",
   "id": "059e4f4f-7d8c-41c1-8120-cceb1d0fb5ac",
   "metadata": {
    "collapsed": false,
    "name": "md_manual_task_execution"
   },
   "source": "## Manual Task Execution\nOur task that pulls data from our stream won't run for another hour. Let's first check the stream for data (there should be some there), and then we'll execute the task to get data into snowflake now."
  },
  {
   "cell_type": "code",
   "id": "88760fc2-9f45-4666-8bb6-2a5b853e7941",
   "metadata": {
    "language": "sql",
    "name": "sql_execute_download_pdf",
    "collapsed": false
   },
   "outputs": [],
   "source": "EXECUTE TASK download_fed_pdf_to_stage;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c66305af-c51f-456f-b140-51ba511b946b",
   "metadata": {
    "name": "md_monitor_execution",
    "collapsed": false
   },
   "source": "### Monitor Execution\nYou can check the task executions by following these instructions: https://docs.snowflake.com/en/user-guide/ui-snowsight-tasks#view-and-manage-individual-tasks. \n\nOnce you see that the task is running, you should be able to query that stage location and see some files landing in the stage with the following SQL."
  },
  {
   "cell_type": "code",
   "id": "ada3cfae-89d4-419b-8c9f-41c1ad8626d8",
   "metadata": {
    "language": "sql",
    "name": "sql_monitor_execution",
    "collapsed": false
   },
   "outputs": [],
   "source": "ALTER STAGE FED_PDF REFRESH;\nSELECT * FROM DIRECTORY (@FED_PDF);",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c0433ad0-17d3-49e7-bfc5-12ebdb8c95d6",
   "metadata": {
    "name": "md_check_stream",
    "collapsed": false
   },
   "source": "### Check the Stream\nRemember the stream is monitoring for new files in our stage location. Let's take a look at that stage to see if it has any data in it."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dd1493-c2fa-4c69-808a-120fcc2c077a",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "sql_check_stream",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "--first, let's check the stream.\nALTER STAGE FED_PDF REFRESH;\nSELECT * FROM FOMC_STREAM WHERE METADATA$ACTION='INSERT';"
  },
  {
   "cell_type": "markdown",
   "id": "87e1e42e-2a6c-4915-be96-20fcf9ac278f",
   "metadata": {
    "name": "md_manual_task_execution_2",
    "collapsed": false
   },
   "source": "### Manual Task Execution\nGreat, the data is in our internal stage and our stream has captured that new data. Let's manually execute the LOAD_FED_PDFS_STAGE_TO_TABLE task to do the real magic and bring in the PDF text along with our hawkish, dovish, or neutral sentiment.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274f603e-e0a2-4b6d-9860-0eacce5c8902",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "sql_manually_execute_LOADPDFS_task"
   },
   "outputs": [],
   "source": "--now let's manually execute the script\n--!! remember, if there was data in the above, it will run, if not, it will not run.\nEXECUTE TASK LOAD_FED_PDFS_STAGE_TO_TABLE;"
  },
  {
   "cell_type": "markdown",
   "id": "20827436-a2e7-4b3e-8734-7ea4d8f530cd",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "### Inspect Output\nLooking at all of the data we can see our Sentiment column with our json object."
  },
  {
   "cell_type": "code",
   "id": "6a573fb5-c885-468a-b594-1b8b718c84e9",
   "metadata": {
    "language": "sql",
    "name": "sql_check_out_the_data",
    "collapsed": false
   },
   "outputs": [],
   "source": "SELECT * FROM PDF_FULL_TEXT;",
   "execution_count": null
  }
 ]
}
